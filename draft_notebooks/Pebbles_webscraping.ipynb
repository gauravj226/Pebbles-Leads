{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#All events in a single file"
      ],
      "metadata": {
        "id": "CYl7zEfSAoDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Base URL\n",
        "BASE_URL = \"https://www.essexstudent.com\"\n",
        "\n",
        "def scrape_event_cards():\n",
        "    \"\"\"Fetch and parse event details from the main events page.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(BASE_URL + \"/whatson/\")\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Initialize list to store event details\n",
        "        events = []\n",
        "\n",
        "        # Find all event cards\n",
        "        event_cards = soup.find_all('div', class_='event_item')\n",
        "        print(f\"Found {len(event_cards)} event cards\")\n",
        "\n",
        "        # Get page title for all events\n",
        "        page_title = soup.find('title').get_text(strip=True) if soup.find('title') else ''\n",
        "\n",
        "        for card in event_cards:\n",
        "            event_details = {\n",
        "                'page_title': page_title,\n",
        "                'title': '',\n",
        "                'date': '',\n",
        "                'location': '',\n",
        "                'description': '',\n",
        "                'ticket_details': 'N/A'  # Ticket details not available in cards\n",
        "            }\n",
        "\n",
        "            # Extract details from the card's details div\n",
        "            details_div = card.find('dl')\n",
        "            if details_div:\n",
        "                # Title\n",
        "                title = details_div.find('a', class_='msl_event_name')\n",
        "                event_details['title'] = title.get_text(strip=True) if title else ''\n",
        "\n",
        "\n",
        "                # Date and Time\n",
        "                time = details_div.find('dd', class_='msl_event_time')\n",
        "                if time:\n",
        "                    time_text = time.get_text(strip=True)\n",
        "                    event_details['date'] = time_text  # Store full text in date\n",
        "                    event_details['time'] = ''  # Leave time empty as it's combined\n",
        "\n",
        "                # Location\n",
        "                location = details_div.find('dd', class_='msl_event_location')\n",
        "                event_details['location'] = location.get_text(strip=True) if location else ''\n",
        "\n",
        "                # Description\n",
        "                description = details_div.find('dd', class_='msl_event_description')\n",
        "                event_details['description'] = description.get_text(strip=True) if description else ''\n",
        "\n",
        "            events.append(event_details)\n",
        "\n",
        "        return events\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching main page: {e}\")\n",
        "        return []\n",
        "\n",
        "def save_to_csv(events):\n",
        "    \"\"\"Save event details to a CSV file.\"\"\"\n",
        "    with open('events.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = ['Page Title', 'Title', 'Date and Time', 'Location', 'Description', 'Ticket Details']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        for event in events:\n",
        "            writer.writerow({\n",
        "                'Page Title': event['page_title'],\n",
        "                'Title': event['title'],\n",
        "                'Date and Time': event['date'],\n",
        "                'Location': event['location'],\n",
        "                'Description': event['description'],\n",
        "                'Ticket Details': event['ticket_details']\n",
        "            })\n",
        "\n",
        "def main():\n",
        "    # Scrape event details from the main page\n",
        "    events = scrape_event_cards()\n",
        "\n",
        "    # Save to CSV\n",
        "    if events:\n",
        "        save_to_csv(events)\n",
        "        print(f\"Saved {len(events)} events to events.csv\")\n",
        "    else:\n",
        "        print(\"No events found or error occurred\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FWcDR7yOuXI",
        "outputId": "71785a6b-c281-4fbf-8126-ce0c35480777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 97 event cards\n",
            "Saved 97 events to events.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean Data\n",
        "- Merge recurring events\n",
        "--- Single titles\n",
        "--- Multiple dates, multiple venues\n",
        "\n",
        "- Separate date from time\n",
        "-- Synonymise dates\n",
        "\n",
        "\n",
        "- Create a composite description column"
      ],
      "metadata": {
        "id": "uNNk0-VyFnj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('events.csv')\n",
        "\n",
        "# Step 1: Merge recurring events by Title\n",
        "def merge_recurring_events(df):\n",
        "    # Group by Title and aggregate necessary fields\n",
        "    merged_df = df.groupby('Title').agg({\n",
        "        'Date and Time': lambda x: '; '.join(x),\n",
        "        'Location': lambda x: ', '.join(sorted(set(x.dropna()))),  # Unique venues, comma-separated\n",
        "        'Description': 'first',  # Take only the first description\n",
        "        'Ticket Details': 'first',  # Take only the first ticket details\n",
        "        'Page Title': 'first'\n",
        "    }).reset_index()\n",
        "    return merged_df\n",
        "\n",
        "df = merge_recurring_events(df)\n",
        "\n",
        "# Step 2: Separate date from time and aggregate them\n",
        "def parse_date_time(date_time_str):\n",
        "    dates = []\n",
        "    times = []\n",
        "    # Split by semicolon first to handle multiple date-time entries\n",
        "    date_time_entries = date_time_str.split(';')\n",
        "\n",
        "    for entry in date_time_entries:\n",
        "        entry = entry.strip()\n",
        "        # Handle multi-day events (e.g., \"20th May noon - 22nd May midnight\")\n",
        "        if '-' in entry:\n",
        "            date_times = entry.split(' - ')\n",
        "            start = date_times[0].strip()\n",
        "            end = date_times[1].strip() if len(date_times) > 1 else None\n",
        "            # Extract date and time for start\n",
        "            start_date_match = re.match(r'(\\d{1,2}(?:st|nd|rd|th)? \\w+)(.*)', start)\n",
        "            if start_date_match:\n",
        "                start_date, start_time = start_date_match.groups()\n",
        "                dates.append(start_date.strip())\n",
        "                times.append(start_time.strip() if start_time else '')\n",
        "            # Extract date and time for end, if present\n",
        "            if end:\n",
        "                end_date_match = re.match(r'(\\d{1,2}(?:st|nd|rd|th)? \\w+)(.*)', end)\n",
        "                if end_date_match:\n",
        "                    end_date, end_time = end_date_match.groups()\n",
        "                    dates.append(end_date.strip())\n",
        "                    times.append(end_time.strip() if end_time else '')\n",
        "        else:\n",
        "            # Handle single-day events\n",
        "            date_time_match = re.match(r'(\\d{1,2}(?:st|nd|rd|th)? \\w+)(.*)', entry)\n",
        "            if date_time_match:\n",
        "                date, time = date_time_match.groups()\n",
        "                dates.append(date.strip())\n",
        "                times.append(time.strip() if time else '')\n",
        "\n",
        "    return dates, times\n",
        "\n",
        "# Apply date-time parsing and aggregate\n",
        "def aggregate_dates_times(df):\n",
        "    date_time_info = df['Date and Time'].apply(parse_date_time)\n",
        "    df['Dates'] = date_time_info.apply(lambda x: ', '.join([d for d in x[0] if d]))\n",
        "    df['Times'] = date_time_info.apply(lambda x: ', '.join([t for t in x[1] if t]))\n",
        "    return df\n",
        "\n",
        "df = aggregate_dates_times(df)\n",
        "\n",
        "# Step 3: Standardize dates\n",
        "def standardize_date(date_str):\n",
        "    if not date_str:\n",
        "        return None\n",
        "    try:\n",
        "        # Remove ordinal indicators (st, nd, rd, th)\n",
        "        date_str = re.sub(r'(\\d{1,2})(st|nd|rd|th)', r'\\1', date_str)\n",
        "        # Parse date assuming year is 2025\n",
        "        parsed_date = datetime.strptime(date_str + ' 2025', '%d %B %Y')\n",
        "        # Get day as integer\n",
        "        day = parsed_date.day\n",
        "        # Determine ordinal suffix\n",
        "        if 10 <= day % 100 <= 20:\n",
        "            suffix = 'th'\n",
        "        else:\n",
        "            suffix = {1: 'st', 2: 'nd', 3: 'rd'}.get(day % 10, 'th')\n",
        "        # Format date as \"day ordinal month year\" (e.g., 2nd June 2025)\n",
        "        return parsed_date.strftime(f'%d{suffix} %B %Y')\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Apply standardization to each date in the Dates column\n",
        "df['Dates'] = df['Dates'].apply(lambda x: ', '.join([standardize_date(d) for d in x.split(', ') if d]))\n",
        "\n",
        "# Step 4: Create composite description column\n",
        "def create_composite_description(row):\n",
        "    components = [row['Title']]\n",
        "    if pd.notna(row['Description']) and row['Description']:\n",
        "        components.append(row['Description'])\n",
        "    if pd.notna(row['Ticket Details']) and row['Ticket Details']:\n",
        "        components.append(row['Ticket Details'])\n",
        "    return ' | '.join(components)\n",
        "\n",
        "df['Composite Description'] = df.apply(create_composite_description, axis=1)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df = df.drop(columns=['Date and Time', 'Ticket Details'])\n",
        "\n",
        "# Reorder columns for clarity\n",
        "df = df[['Title', 'Composite Description', 'Location', 'Dates', 'Times', 'Page Title']]\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "df.to_csv('cleaned_events.csv', index=False)\n",
        "\n",
        "# Display the first few rows of the cleaned dataframe\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15i5x8w0H2ZM",
        "outputId": "660d8d8f-5f17-4fb5-aef3-a9e784f82ac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                  Title  \\\n",
            "0                        7th SU Council   \n",
            "1            A Rebel's Guide to Gramsci   \n",
            "2                         All Out Essex   \n",
            "3              Alwan Society Take Over!   \n",
            "4  Anti-Social & Introverts Social Club   \n",
            "\n",
            "                               Composite Description Location  \\\n",
            "0  7th SU Council | Elected representatives from ...            \n",
            "1  A Rebel's Guide to Gramsci | The Italian Marxi...            \n",
            "2  All Out Essex | An end-of-year talent showcase...            \n",
            "3  Alwan Society Take Over! | Create your event, ...            \n",
            "4  Anti-Social & Introverts Social Club | A low-k...            \n",
            "\n",
            "                           Dates           Times  Page Title  \n",
            "0   20th May 2025, 22nd May 2025  6:30pm, 1:15pm         NaN  \n",
            "1                  13th May 2025             7pm         NaN  \n",
            "2                  14th May 2025          6:30pm         NaN  \n",
            "3                  24th May 2025             9pm         NaN  \n",
            "4  15th May 2025, 10th June 2025        6pm, 6pm         NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Events by month"
      ],
      "metadata": {
        "id": "8VQ0vmLDAf1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Base URL\n",
        "BASE_URL = \"https://www.essexstudent.com\"\n",
        "\n",
        "def parse_month(date_string):\n",
        "    \"\"\"Extract the month name from a date string.\"\"\"\n",
        "    # Common date formats: \"January 15, 2025\", \"15 Jan 2025\", \"Jan 15, 2025\"\n",
        "    try:\n",
        "        # Replace multiple spaces and normalize\n",
        "        date_string = re.sub(r'\\s+', ' ', date_string.strip())\n",
        "        # Try parsing with different formats\n",
        "        for fmt in (\n",
        "            \"%B %d, %Y\", \"%d %B %Y\", \"%b %d, %Y\", \"%d %b %Y\",\n",
        "            \"%B %d %Y\", \"%d %B, %Y\", \"%b %d %Y\", \"%d %b, %Y\"\n",
        "        ):\n",
        "            try:\n",
        "                date_obj = datetime.strptime(date_string, fmt)\n",
        "                return date_obj.strftime(\"%B_%Y\")  # e.g., \"January_2025\"\n",
        "            except ValueError:\n",
        "                continue\n",
        "        # Fallback: look for month names or abbreviations\n",
        "        month_names = (\n",
        "            r'january|february|march|april|may|june|july|august|september|october|november|december|'\n",
        "            r'jan|feb|mar|apr|jun|jul|aug|sep|oct|nov|dec'\n",
        "        )\n",
        "        match = re.search(month_names, date_string.lower())\n",
        "        if match:\n",
        "            month = match.group()\n",
        "            # Map abbreviations to full month names\n",
        "            month_map = {\n",
        "                'jan': 'January', 'feb': 'February', 'mar': 'March', 'apr': 'April',\n",
        "                'may': 'May', 'jun': 'June', 'jul': 'July', 'aug': 'August',\n",
        "                'sep': 'September', 'oct': 'October', 'nov': 'November', 'dec': 'December'\n",
        "            }\n",
        "            full_month = month_map.get(month.lower(), month.capitalize())\n",
        "            # Try to extract year\n",
        "            year_match = re.search(r'\\d{4}', date_string)\n",
        "            year = year_match.group() if year_match else datetime.now().strftime(\"%Y\")\n",
        "            return f\"{full_month}_{year}\"\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing date '{date_string}': {e}\")\n",
        "        return None\n",
        "\n",
        "def scrape_event_cards():\n",
        "    \"\"\"Fetch and parse event details from the main events page.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(BASE_URL + \"/whatson/\")\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Initialize list to store event details\n",
        "        events = []\n",
        "\n",
        "        # Find all event cards\n",
        "        event_cards = soup.find_all('div', class_='event_item')\n",
        "        print(f\"Found {len(event_cards)} event cards\")\n",
        "\n",
        "        # Get page title for all events\n",
        "        page_title = soup.find('title').get_text(strip=True) if soup.find('title') else ''\n",
        "\n",
        "        for card in event_cards:\n",
        "            event_details = {\n",
        "                'page_title': page_title,\n",
        "                'title': '',\n",
        "                'date': '',\n",
        "                'location': '',\n",
        "                'description': '',\n",
        "                'ticket_details': 'N/A',\n",
        "                'month_year': ''  # To store parsed month and year\n",
        "            }\n",
        "\n",
        "            # Extract details from the card's details div\n",
        "            details_div = card.find('dl')\n",
        "            if details_div:\n",
        "                # Title\n",
        "                title = details_div.find('a', class_='msl_event_name')\n",
        "                event_details['title'] = title.get_text(strip=True) if title else ''\n",
        "\n",
        "                # Date and Time\n",
        "                time = details_div.find('dd', class_='msl_event_time')\n",
        "                if time:\n",
        "                    time_text = time.get_text(strip=True)\n",
        "                    event_details['date'] = time_text\n",
        "                    event_details['month_year'] = parse_month(time_text) or 'Unknown'\n",
        "\n",
        "                # Location\n",
        "                location = details_div.find('dd', class_='msl_event_location')\n",
        "                event_details['location'] = location.get_text(strip=True) if location else ''\n",
        "\n",
        "                # Description\n",
        "                description = details_div.find('dd', class_='msl_event_description')\n",
        "                event_details['description'] = description.get_text(strip=True) if description else ''\n",
        "\n",
        "            events.append(event_details)\n",
        "\n",
        "        return events\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching main page: {e}\")\n",
        "        return []\n",
        "\n",
        "def save_to_csv(events):\n",
        "    \"\"\"Save event details to separate CSV files by month.\"\"\"\n",
        "    # Group events by month_year\n",
        "    events_by_month = {}\n",
        "    for event in events:\n",
        "        month_year = event['month_year']\n",
        "        if month_year not in events_by_month:\n",
        "            events_by_month[month_year] = []\n",
        "        events_by_month[month_year].append(event)\n",
        "\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs('events_by_month', exist_ok=True)\n",
        "\n",
        "    # Save each month's events to a separate CSV\n",
        "    fieldnames = ['Page Title', 'Title', 'Date and Time', 'Location', 'Description', 'Ticket Details']\n",
        "    for month_year, month_events in events_by_month.items():\n",
        "        filename = f\"events_by_month/events_{month_year}.csv\"\n",
        "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            for event in month_events:\n",
        "                writer.writerow({\n",
        "                    'Page Title': event['page_title'],\n",
        "                    'Title': event['title'],\n",
        "                    'Date and Time': event['date'],\n",
        "                    'Location': event['location'],\n",
        "                    'Description': event['description'],\n",
        "                    'Ticket Details': event['ticket_details']\n",
        "                })\n",
        "        print(f\"Saved {len(month_events)} events to {filename}\")\n",
        "\n",
        "def main():\n",
        "    # Scrape event details from the main page\n",
        "    events = scrape_event_cards()\n",
        "\n",
        "    # Save to CSV by month\n",
        "    if events:\n",
        "        save_to_csv(events)\n",
        "        print(f\"Processed {len(events)} events into monthly CSV files\")\n",
        "    else:\n",
        "        print(\"No events found or error occurred\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jcTuOdeAbcf",
        "outputId": "a9334b97-2eb1-4b09-fb65-40c43e41b277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 118 event cards\n",
            "Saved 1 events to events_by_month/events_April_2025.csv\n",
            "Saved 61 events to events_by_month/events_May_2025.csv\n",
            "Saved 42 events to events_by_month/events_June_2025.csv\n",
            "Saved 8 events to events_by_month/events_July_2025.csv\n",
            "Saved 4 events to events_by_month/events_August_2025.csv\n",
            "Saved 2 events to events_by_month/events_September_2025.csv\n",
            "Processed 118 events into monthly CSV files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hzgQFFCrAcEN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}